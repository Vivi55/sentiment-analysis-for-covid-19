{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7680</th>\n",
       "      <td>7654</td>\n",
       "      <td>#RIPTwitter #GASLIGHTER #lauvxbts√Ç¬† #Coronavir...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7681</th>\n",
       "      <td>7655</td>\n",
       "      <td>In times like this if you√¢¬Ä¬ôre ever about to g...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7682</th>\n",
       "      <td>7656</td>\n",
       "      <td>@AliceEvansGruff @AngelaBelcamino Don't you th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7683</th>\n",
       "      <td>7657</td>\n",
       "      <td>Great idea! Bart needs to do something. Bart, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7684</th>\n",
       "      <td>7658</td>\n",
       "      <td>@atrupar He√¢¬Ä¬ôs been in office for 3 years, gu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27817</th>\n",
       "      <td>27747</td>\n",
       "      <td>@TamarHaspel Before you tweet again on this, r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27818</th>\n",
       "      <td>27748</td>\n",
       "      <td>Mal etwas zum schmunzeln √∞¬ü¬ò¬ä bei all dem Schr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27819</th>\n",
       "      <td>27749</td>\n",
       "      <td>#√ö¬©√ò¬±√ô¬à√ô¬Ü√ò¬ß √ò¬®√ô¬á √ò¬¨√ò¬ß√ô¬Ü √ô¬Ö",
       "√ô¬Ñ√ò¬™√ò¬å √ò¬ß√ô¬Ü√ò¬Ø√ò¬ß√ò¬Æ√ò¬™√ô...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27820</th>\n",
       "      <td>27750</td>\n",
       "      <td>Would you compensate the ordinary citizens who...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27821</th>\n",
       "      <td>27751</td>\n",
       "      <td>#Covid_19 #Coronavirus #CoronavirusOregon #Cor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20142 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ItemID                                      SentimentText Unnamed: 2  \\\n",
       "7680    7654  #RIPTwitter #GASLIGHTER #lauvxbts√Ç¬† #Coronavir...        NaN   \n",
       "7681    7655  In times like this if you√¢¬Ä¬ôre ever about to g...        NaN   \n",
       "7682    7656  @AliceEvansGruff @AngelaBelcamino Don't you th...        NaN   \n",
       "7683    7657  Great idea! Bart needs to do something. Bart, ...        NaN   \n",
       "7684    7658  @atrupar He√¢¬Ä¬ôs been in office for 3 years, gu...        NaN   \n",
       "...      ...                                                ...        ...   \n",
       "27817  27747  @TamarHaspel Before you tweet again on this, r...        NaN   \n",
       "27818  27748  Mal etwas zum schmunzeln √∞¬ü¬ò¬ä bei all dem Schr...        NaN   \n",
       "27819  27749  #√ö¬©√ò¬±√ô¬à√ô¬Ü√ò¬ß √ò¬®√ô¬á √ò¬¨√ò¬ß√ô¬Ü √ô\n",
       "√ô¬Ñ√ò¬™√ò¬å √ò¬ß√ô¬Ü√ò¬Ø√ò¬ß√ò¬Æ√ò¬™√ô...        NaN   \n",
       "27820  27750  Would you compensate the ordinary citizens who...        NaN   \n",
       "27821  27751  #Covid_19 #Coronavirus #CoronavirusOregon #Cor...        NaN   \n",
       "\n",
       "      Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8  \\\n",
       "7680         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "7681         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "7682         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "7683         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "7684         NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "27817        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "27818        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "27819        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "27820        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "27821        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "      Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13  \n",
       "7680         NaN         NaN         NaN         NaN         NaN  \n",
       "7681         NaN         NaN         NaN         NaN         NaN  \n",
       "7682         NaN         NaN         NaN         NaN         NaN  \n",
       "7683         NaN         NaN         NaN         NaN         NaN  \n",
       "7684         NaN         NaN         NaN         NaN         NaN  \n",
       "...          ...         ...         ...         ...         ...  \n",
       "27817        NaN         NaN         NaN         NaN         NaN  \n",
       "27818        NaN         NaN         NaN         NaN         NaN  \n",
       "27819        NaN         NaN         NaN         NaN         NaN  \n",
       "27820        NaN         NaN         NaN         NaN         NaN  \n",
       "27821        NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[20142 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data0305=pd.read_csv(r'./20200305 Coronavirus Tweets.CSV', encoding='ISO-8859-1',low_memory=False)\n",
    "newdata=data0305[7680:27822]\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7680</th>\n",
       "      <td>2020-03-05T00:00:00Z</td>\n",
       "      <td>Taking a closer look at the profiles of the vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7681</th>\n",
       "      <td>2020-03-05T00:00:24Z</td>\n",
       "      <td>Global Economy:\\n\\nPrevious #coronavirus strai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7682</th>\n",
       "      <td>2020-03-05T00:00:25Z</td>\n",
       "      <td>@Uber @UberFacts @Ola_Delhi\\n@Olacabs\\nAs #Cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7683</th>\n",
       "      <td>2020-03-05T00:00:25Z</td>\n",
       "      <td>Up to 100,000 Israelis in isolation as Israel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7684</th>\n",
       "      <td>2020-03-05T00:00:25Z</td>\n",
       "      <td>CASHüíµcan b Contaminated ü¶†üò∑ ACCEPTING CASH APP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27817</th>\n",
       "      <td>2020-03-05T23:59:27Z</td>\n",
       "      <td>With the amount of vagrants in LA County, coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27818</th>\n",
       "      <td>2020-03-05T23:59:31Z</td>\n",
       "      <td>@FlossObama Nah dude he's got it figured out. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27819</th>\n",
       "      <td>2020-03-05T23:59:45Z</td>\n",
       "      <td>The #coronavirus #COVID„Éº19 #covid_19 effect vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27820</th>\n",
       "      <td>2020-03-05T23:59:49Z</td>\n",
       "      <td>AA waiving change fees for new reservations bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27821</th>\n",
       "      <td>2020-03-05T23:59:51Z</td>\n",
       "      <td>‰∏ñ„ÅÆ‰∏≠„ÅåÊöó„ÅÑÁ©∫Ê∞ó„Å´ÂåÖ„Åæ„Çå„Åü‰ªä„Å†„Åã„ÇâÁöÜ„Åï„Çì„Å´„Åì„ÅÆÊõ≤„ÇíÊçß„Åí„Åæ„Åô„ÄÇ\\nÊòéÊó•„Å∏„ÅÆÂ∏åÊúõ„Å®ÂÖâ„Çä„ÅÆÊõ≤„ÄÇ\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20142 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                                               text\n",
       "7680   2020-03-05T00:00:00Z  Taking a closer look at the profiles of the vi...\n",
       "7681   2020-03-05T00:00:24Z  Global Economy:\\n\\nPrevious #coronavirus strai...\n",
       "7682   2020-03-05T00:00:25Z  @Uber @UberFacts @Ola_Delhi\\n@Olacabs\\nAs #Cor...\n",
       "7683   2020-03-05T00:00:25Z  Up to 100,000 Israelis in isolation as Israel ...\n",
       "7684   2020-03-05T00:00:25Z  CASHüíµcan b Contaminated ü¶†üò∑ ACCEPTING CASH APP ...\n",
       "...                     ...                                                ...\n",
       "27817  2020-03-05T23:59:27Z  With the amount of vagrants in LA County, coro...\n",
       "27818  2020-03-05T23:59:31Z  @FlossObama Nah dude he's got it figured out. ...\n",
       "27819  2020-03-05T23:59:45Z  The #coronavirus #COVID„Éº19 #covid_19 effect vi...\n",
       "27820  2020-03-05T23:59:49Z  AA waiving change fees for new reservations bu...\n",
       "27821  2020-03-05T23:59:51Z  ‰∏ñ„ÅÆ‰∏≠„ÅåÊöó„ÅÑÁ©∫Ê∞ó„Å´ÂåÖ„Åæ„Çå„Åü‰ªä„Å†„Åã„ÇâÁöÜ„Åï„Çì„Å´„Åì„ÅÆÊõ≤„ÇíÊçß„Åí„Åæ„Åô„ÄÇ\\nÊòéÊó•„Å∏„ÅÆÂ∏åÊúõ„Å®ÂÖâ„Çä„ÅÆÊõ≤„ÄÇ\\n...\n",
       "\n",
       "[20142 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "march05data=march12data[[\"created_at\",\"text\"]]\n",
    "march05data[7680:27822]# 20142 rows √ó 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-04T11:09:52Z</td>\n",
       "      <td>#CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-04T11:09:52Z</td>\n",
       "      <td>#CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-04T11:09:57Z</td>\n",
       "      <td>China‚Äôs wildlife trade via @SCMPgraphics  http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-04T11:10:09Z</td>\n",
       "      <td>Proof that the media such as @CNN is unnecessa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-04T11:10:21Z</td>\n",
       "      <td>Indians are a shitty breed of humans. With the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             created_at                                               text\n",
       "0  2020-03-04T11:09:52Z  #CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...\n",
       "1  2020-03-04T11:09:52Z  #CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...\n",
       "2  2020-03-04T11:09:57Z  China‚Äôs wildlife trade via @SCMPgraphics  http...\n",
       "3  2020-03-04T11:10:09Z  Proof that the media such as @CNN is unnecessa...\n",
       "4  2020-03-04T11:10:21Z  Indians are a shitty breed of humans. With the..."
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "march05data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy emoticons: {':)', 'xd', ';)', ':D', ';D', ':-D', ':p', ':P', ';-)', 'x)', 'X)', 'XD', ':-)', 'xD'}\n",
      "Sad emoticons: {':(', \":'(\", ':|', ':/'}\n"
     ]
    }
   ],
   "source": [
    "#Emoticons\n",
    "import re\n",
    "tweets_text = march05data.text.str.cat()\n",
    "emos = set(re.findall(r\" ([xX:;][-']?.) \",tweets_text))\n",
    "emos_count = []\n",
    "for emo in emos:\n",
    "    emos_count.append((tweets_text.count(emo), emo))\n",
    "#print(sorted(emos_count,reverse=True))\n",
    "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n",
    "SAD_EMO = r\" (:'?[/|\\(]) \"\n",
    "print(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\n",
    "print(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\wyvva\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Sentiment'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-7992419db7ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0msentiments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarch05data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarch05data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SentimentText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wyvva\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wyvva\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Sentiment'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer, punkt, TweetTokenizer\n",
    "from nltk.corpus import stopwords,state_union\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def most_used_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "    print(\"There is %d different words\" % len(set(tokens)))\n",
    "    return sorted(frequency_dist, key=frequency_dist.__getitem__, reverse=True)\n",
    "\n",
    "def stop_words():\n",
    "    mw = most_used_words(train_data.text.str.cat())\n",
    "    most_words = []\n",
    "    for w in mw:\n",
    "        if len(most_words) == 1000:\n",
    "            break\n",
    "        if w in stopwords.words(\"english\"):\n",
    "            continue\n",
    "        else:\n",
    "            most_words.append(w)\n",
    "    print(sorted(most_words))\n",
    "\n",
    "def stem_tokenize(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    return [stemmer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "def lemmatize_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "class TextPreProc(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, use_mention=False):\n",
    "        self.use_mention = use_mention\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # We can choose between keeping the mentions\n",
    "        # or deleting them\n",
    "        if self.use_mention:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")\n",
    "        else:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n",
    "\n",
    "        # Keeping only the word after the #\n",
    "        X = X.str.replace(\"#\", \"\")\n",
    "        X = X.str.replace(r\"[-\\.\\n]\", \"\")\n",
    "        # Removing HTML garbage\n",
    "        X = X.str.replace(r\"&\\w+;\", \"\")\n",
    "        # Removing links\n",
    "        X = X.str.replace(r\"https?://\\S*\", \"\")\n",
    "        # replace repeated letters with only two occurences\n",
    "        # heeeelllloooo => heelloo\n",
    "        X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n",
    "        # mark emoticons as happy or sad\n",
    "        X = X.str.replace(HAPPY_EMO, \" happyemoticons \")\n",
    "        X = X.str.replace(SAD_EMO, \" sademoticons \")\n",
    "        X = X.str.lower()\n",
    "        return X\n",
    "\n",
    "sentiments = march05data['Sentiment']\n",
    "tweets = march05data['SentimentText']\n",
    "\n",
    "# I get those parameters from the 'Fine tune the model' part\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatize_tokenize, ngram_range=(1,2))\n",
    "pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc(use_mention=True)),\n",
    "    ('vectorizer', vectorizer),\n",
    "])\n",
    "#  split data into learning set and testing set\n",
    "learn_data, test_data, sentiments_learning, sentiments_test = train_test_split(tweets, sentiments, test_size=0.3)\n",
    "learning_data = pipeline.fit_transform(learn_data)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "models = {\n",
    "    'logitic regression': lr,\n",
    "    'bernoulliNB': bnb,\n",
    "    'multinomialNB': mnb,\n",
    "}\n",
    "grid_search_pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc()),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB()),\n",
    "])\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'text_pre_processing__use_mention': [True, False],\n",
    "        'vectorizer__max_features': [1000, 2000, 5000, 10000, 20000, None],\n",
    "        'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "    },\n",
    "]\n",
    "grid_search = GridSearchCV(grid_search_pipeline, params, cv=5, scoring='f1')\n",
    "grid_search.fit(learn_data, sentiments_learning)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "mnb.fit(learning_data, sentiments_learning)\n",
    "testing_data = pipeline.transform(test_data)\n",
    "mnb.score(testing_data, sentiments_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(learning_data, sentiments_learning)\n",
    "tweet = pd.Series([input(),])\n",
    "tweet = pipeline.transform(tweet)\n",
    "proba = model.predict_proba(tweet)[0]\n",
    "print(\"The probability that this tweet is sad is:\", proba[0])\n",
    "print(\"The probability that this tweet is happy is:\", proba[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-6361d2547ebc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotlyNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading plotly-4.6.0-py2.py3-none-any.whl (7.1 MB)\n",
      "Requirement already satisfied: six in c:\\users\\wyvva\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from plotly) (1.14.0)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Installing collected packages: retrying, plotly\n",
      "    Running setup.py install for retrying: started\n",
      "    Running setup.py install for retrying: finished with status 'done'\n",
      "Successfully installed plotly-4.6.0 retrying-1.3.3\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
