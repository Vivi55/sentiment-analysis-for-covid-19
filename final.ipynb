{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer, punkt, TweetTokenizer\n",
    "from nltk.corpus import stopwords,state_union\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 6)\n",
      "(0, 6)\n",
      "(5000, 6)\n",
      "(10000, 6)\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "data = pd.read_csv(\"./training.1600000.processed.noemoticon.csv\",encoding='latin-1')\n",
    "DATASET_COLUMNS = [\"Sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"Text\"]\n",
    "data.columns = DATASET_COLUMNS\n",
    "positif_data = data[data.Sentiment==4].iloc[:5000,:]\n",
    "print(positif_data.shape)\n",
    "neutral_data = data[data.Sentiment==2].iloc[:5000,:]\n",
    "print(neutral_data.shape)\n",
    "negative_data = data[data.Sentiment==0].iloc[:5000,:]\n",
    "print(negative_data.shape)\n",
    "data = pd.concat([positif_data,neutral_data,negative_data],axis = 0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822272</td>\n",
       "      <td>Mon Apr 06 22:22:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ersle</td>\n",
       "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822273</td>\n",
       "      <td>Mon Apr 06 22:22:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>becca210</td>\n",
       "      <td>im meeting up with one of my besties tonight! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822283</td>\n",
       "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Wingman29</td>\n",
       "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822287</td>\n",
       "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>katarinka</td>\n",
       "      <td>Being sick can be really cheap when it hurts t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822293</td>\n",
       "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_EmilyYoung</td>\n",
       "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment         ids                          date      flag  \\\n",
       "799999          4  1467822272  Mon Apr 06 22:22:45 PDT 2009  NO_QUERY   \n",
       "800000          4  1467822273  Mon Apr 06 22:22:45 PDT 2009  NO_QUERY   \n",
       "800001          4  1467822283  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n",
       "800002          4  1467822287  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n",
       "800003          4  1467822293  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n",
       "\n",
       "               user                                               Text  \n",
       "799999        ersle       I LOVE @Health4UandPets u guys r the best!!   \n",
       "800000     becca210  im meeting up with one of my besties tonight! ...  \n",
       "800001    Wingman29  @DaRealSunisaKim Thanks for the Twitter add, S...  \n",
       "800002    katarinka  Being sick can be really cheap when it hurts t...  \n",
       "800003  _EmilyYoung    @LovesBrooklyn2 he has that effect on everyone   "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>Text</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822272</td>\n",
       "      <td>Mon Apr 06 22:22:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ersle</td>\n",
       "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
       "      <td>love health uandpets guys best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822273</td>\n",
       "      <td>Mon Apr 06 22:22:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>becca210</td>\n",
       "      <td>im meeting up with one of my besties tonight! ...</td>\n",
       "      <td>meeting besties tonight cant wait girl talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822283</td>\n",
       "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Wingman29</td>\n",
       "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
       "      <td>darealsunisakim thanks twitter sunisa meet sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822287</td>\n",
       "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>katarinka</td>\n",
       "      <td>Being sick can be really cheap when it hurts t...</td>\n",
       "      <td>sick really cheap hurts much real food plus fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822293</td>\n",
       "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_EmilyYoung</td>\n",
       "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
       "      <td>lovesbrooklyn effect everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0</td>\n",
       "      <td>1468963112</td>\n",
       "      <td>Tue Apr 07 04:36:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>itikecil</td>\n",
       "      <td>a friend broke his promises..</td>\n",
       "      <td>friend broke promises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>1468963184</td>\n",
       "      <td>Tue Apr 07 04:36:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JoanneDuran</td>\n",
       "      <td>@gjarnling I am fine thanks - tired</td>\n",
       "      <td>gjarnling fine thanks tired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0</td>\n",
       "      <td>1468963325</td>\n",
       "      <td>Tue Apr 07 04:36:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ewinsk</td>\n",
       "      <td>trying to keep my eyes open..damn baking</td>\n",
       "      <td>trying keep eyes open damn baking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>1468963547</td>\n",
       "      <td>Tue Apr 07 04:36:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Abbeyluvselvie</td>\n",
       "      <td>why the hell is it snowing</td>\n",
       "      <td>hell snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>1468963818</td>\n",
       "      <td>Tue Apr 07 04:36:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Nezbo</td>\n",
       "      <td>@LouLouK Gisburn is great, there are some nice...</td>\n",
       "      <td>loulouk gisburn great nice little single track...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment         ids                          date      flag  \\\n",
       "799999          4  1467822272  Mon Apr 06 22:22:45 PDT 2009  NO_QUERY   \n",
       "800000          4  1467822273  Mon Apr 06 22:22:45 PDT 2009  NO_QUERY   \n",
       "800001          4  1467822283  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n",
       "800002          4  1467822287  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n",
       "800003          4  1467822293  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n",
       "...           ...         ...                           ...       ...   \n",
       "4995            0  1468963112  Tue Apr 07 04:36:43 PDT 2009  NO_QUERY   \n",
       "4996            0  1468963184  Tue Apr 07 04:36:43 PDT 2009  NO_QUERY   \n",
       "4997            0  1468963325  Tue Apr 07 04:36:46 PDT 2009  NO_QUERY   \n",
       "4998            0  1468963547  Tue Apr 07 04:36:50 PDT 2009  NO_QUERY   \n",
       "4999            0  1468963818  Tue Apr 07 04:36:55 PDT 2009  NO_QUERY   \n",
       "\n",
       "                  user                                               Text  \\\n",
       "799999           ersle       I LOVE @Health4UandPets u guys r the best!!    \n",
       "800000        becca210  im meeting up with one of my besties tonight! ...   \n",
       "800001       Wingman29  @DaRealSunisaKim Thanks for the Twitter add, S...   \n",
       "800002       katarinka  Being sick can be really cheap when it hurts t...   \n",
       "800003     _EmilyYoung    @LovesBrooklyn2 he has that effect on everyone    \n",
       "...                ...                                                ...   \n",
       "4995          itikecil                     a friend broke his promises..    \n",
       "4996       JoanneDuran               @gjarnling I am fine thanks - tired    \n",
       "4997            ewinsk          trying to keep my eyes open..damn baking    \n",
       "4998    Abbeyluvselvie                        why the hell is it snowing    \n",
       "4999             Nezbo  @LouLouK Gisburn is great, there are some nice...   \n",
       "\n",
       "                                            SentimentText  \n",
       "799999                     love health uandpets guys best  \n",
       "800000        meeting besties tonight cant wait girl talk  \n",
       "800001  darealsunisakim thanks twitter sunisa meet sho...  \n",
       "800002  sick really cheap hurts much real food plus fr...  \n",
       "800003                      lovesbrooklyn effect everyone  \n",
       "...                                                   ...  \n",
       "4995                                friend broke promises  \n",
       "4996                          gjarnling fine thanks tired  \n",
       "4997                    trying keep eyes open damn baking  \n",
       "4998                                         hell snowing  \n",
       "4999    loulouk gisburn great nice little single track...  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Twitter Handles (@user)\n",
    "data['SentimentText'] = data['Text'].str.replace(\"@\", \"\")\n",
    "# Removing links\n",
    "data['SentimentText'] = data['SentimentText'].str.replace(r\"http\\S+\", \"\")\n",
    "# Removing Punctuations, Numbers, and Special Characters\n",
    "data['SentimentText'] = data['SentimentText'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# Remove stop words\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    clean_text=' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return clean_text\n",
    "data['SentimentText'] = data['SentimentText'].apply(lambda text : remove_stopwords(text.lower()))\n",
    "# Text Tokenization and Normalization\n",
    "data['SentimentText'] = data['SentimentText'].apply(lambda x: nltk.word_tokenize(x))\n",
    "# Now let’s stitch these tokens back together\n",
    "data['SentimentText'] = data['SentimentText'].apply(lambda x: ' '.join([w for w in x]))\n",
    "# Removing small words\n",
    "data['SentimentText'] = data['SentimentText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "cv = count_vectorizer.fit_transform(data['SentimentText'])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatize_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d3d5793ab317>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SentimentText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlemmatize_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m pipeline = Pipeline([\n\u001b[0;32m     36\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'text_pre_processing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTextPreProc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_mention\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatize_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "def most_used_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "    print(\"There is %d different words\" % len(set(tokens)))\n",
    "    return sorted(frequency_dist, key=frequency_dist.__getitem__, reverse=True)\n",
    "\n",
    "def stop_words():\n",
    "    mw = most_used_words(train_data.text.str.cat())\n",
    "    most_words = []\n",
    "    for w in mw:\n",
    "        if len(most_words) == 1000:\n",
    "            break\n",
    "        if w in stopwords.words(\"english\"):\n",
    "            continue\n",
    "        else:\n",
    "            most_words.append(w)\n",
    "    print(sorted(most_words))\n",
    "\n",
    "def stem_tokenize(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    return [stemmer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "def lemmatize_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]\n",
    "class TextPreProc(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, use_mention=False):\n",
    "        self.use_mention = use_mention\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # We can choose between keeping the mentions\n",
    "        # or deleting them\n",
    "        if self.use_mention:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")\n",
    "        else:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n",
    "\n",
    "        # Keeping only the word after the #\n",
    "        X = X.str.replace(\"#\", \"\")\n",
    "        X = X.str.replace(r\"[-\\.\\n]\", \"\")\n",
    "        # Removing HTML garbage\n",
    "        X = X.str.replace(r\"&\\w+;\", \"\")\n",
    "        # Removing links\n",
    "        X = X.str.replace(r\"https?://\\S*\", \"\")\n",
    "        # replace repeated letters with only two occurences\n",
    "        # heeeelllloooo => heelloo\n",
    "        X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n",
    "        # mark emoticons as happy or sad\n",
    "        #X = X.str.replace(HAPPY_EMO, \" happyemoticons \")\n",
    "        #X = X.str.replace(SAD_EMO, \" sademoticons \")\n",
    "        #X = X.str.lower()\n",
    "        return X\n",
    "sentiments = data['Sentiment']\n",
    "tweets = data['SentimentText']\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatize_tokenize, ngram_range=(1,2))\n",
    "pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc(use_mention=True)),\n",
    "    ('vectorizer', vectorizer),\n",
    "])\n",
    "#  split data into learning set and testing set\n",
    "learn_data, test_data, sentiments_learning, sentiments_test = train_test_split(tweets, sentiments, test_size=0.3)\n",
    "learning_data = pipeline.fit_transform(learn_data)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "models = {\n",
    "    'logitic regression': lr,\n",
    "    'bernoulliNB': bnb,\n",
    "    'multinomialNB': mnb,\n",
    "}\n",
    "grid_search_pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc()),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB()),\n",
    "])\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'text_pre_processing__use_mention': [True, False],\n",
    "        'vectorizer__max_features': [1000, 2000, 5000, 10000, 20000, None],\n",
    "        'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "    },\n",
    "]\n",
    "grid_search = GridSearchCV(grid_search_pipeline, params, cv=5, scoring='f1')\n",
    "grid_search.fit(learn_data, sentiments_learning)\n",
    "print(grid_search.best_params_)\n",
    "mnb.fit(learning_data, sentiments_learning)\n",
    "testing_data = pipeline.transform(test_data)\n",
    "\n",
    "data0328=pd.read_csv('./20200328 Coronavirus Tweets.CSV', encoding='ISO-8859-1',low_memory=False)\n",
    "sub_learning= pipeline.transform(data0328['SentimentText'].apply(lambda sub_learning: np.str_(sub_learning05)))\n",
    "\n",
    "sub = pd.DataFrame(sub_data['ItemID'], columns=(\"ItemID\", \"Sentiment\"))\n",
    "sub[\"Sentiment\"] = mnb.predict(sub_learning)\n",
    "print(sub)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(learning_data, sentiments_learning)\n",
    "tweet = pd.Series([input(),])\n",
    "tweet = pipeline.transform(tweet)\n",
    "proba = model.predict_proba(tweet)[0]\n",
    "print(\"The probability that this tweet is sad is:\", proba[0])\n",
    "print(\"The probability that this tweet is happy is:\", proba[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>#CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>#CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Chinaâs wildlife trade via @SCMPgraphics  ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Proof that the media such as @CNN is unnecessa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Indians are a shitty breed of humans. With the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528303</th>\n",
       "      <td>526787</td>\n",
       "      <td>A really great visual explanation of the pande...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528304</th>\n",
       "      <td>526788</td>\n",
       "      <td>Los casos de COVID-19 se registran mayoritaria...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528305</th>\n",
       "      <td>526789</td>\n",
       "      <td>Apa itu Virus Coronavirus (Covid-19)?\\n\\nSumbe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528306</th>\n",
       "      <td>526790</td>\n",
       "      <td>Folks upset at the public preparedness measure...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528307</th>\n",
       "      <td>526791</td>\n",
       "      <td>See that it is empty Italy and the governmentâ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>528308 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ItemID                                      SentimentText Unnamed: 2  \\\n",
       "0            1  #CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...        NaN   \n",
       "1            2  #CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...        NaN   \n",
       "2            3  Chinaâs wildlife trade via @SCMPgraphics  ht...        NaN   \n",
       "3            4  Proof that the media such as @CNN is unnecessa...        NaN   \n",
       "4            5  Indians are a shitty breed of humans. With the...        NaN   \n",
       "...        ...                                                ...        ...   \n",
       "528303  526787  A really great visual explanation of the pande...        NaN   \n",
       "528304  526788  Los casos de COVID-19 se registran mayoritaria...        NaN   \n",
       "528305  526789  Apa itu Virus Coronavirus (Covid-19)?\\n\\nSumbe...        NaN   \n",
       "528306  526790  Folks upset at the public preparedness measure...        NaN   \n",
       "528307  526791  See that it is empty Italy and the governmentâ...        NaN   \n",
       "\n",
       "       Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8  \\\n",
       "0             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "528303        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528304        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528305        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528306        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528307        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "       Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13  \n",
       "0             NaN         NaN         NaN         NaN         NaN  \n",
       "1             NaN         NaN         NaN         NaN         NaN  \n",
       "2             NaN         NaN         NaN         NaN         NaN  \n",
       "3             NaN         NaN         NaN         NaN         NaN  \n",
       "4             NaN         NaN         NaN         NaN         NaN  \n",
       "...           ...         ...         ...         ...         ...  \n",
       "528303        NaN         NaN         NaN         NaN         NaN  \n",
       "528304        NaN         NaN         NaN         NaN         NaN  \n",
       "528305        NaN         NaN         NaN         NaN         NaN  \n",
       "528306        NaN         NaN         NaN         NaN         NaN  \n",
       "528307        NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[528308 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data0305=pd.read_csv('./20200305 Coronavirus Tweets.CSV', encoding='ISO-8859-1',low_memory=False)\n",
    "data0305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>#CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>#CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Chinaâs wildlife trade via @SCMPgraphics  ht...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Proof that the media such as @CNN is unnecessa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Indians are a shitty breed of humans. With the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528303</th>\n",
       "      <td>526787</td>\n",
       "      <td>A really great visual explanation of the pande...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528304</th>\n",
       "      <td>526788</td>\n",
       "      <td>Los casos de COVID-19 se registran mayoritaria...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528305</th>\n",
       "      <td>526789</td>\n",
       "      <td>Apa itu Virus Coronavirus (Covid-19)?\\n\\nSumbe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528306</th>\n",
       "      <td>526790</td>\n",
       "      <td>Folks upset at the public preparedness measure...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528307</th>\n",
       "      <td>526791</td>\n",
       "      <td>See that it is empty Italy and the governmentâ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>528308 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ItemID                                      SentimentText Unnamed: 2  \\\n",
       "0            1  #CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...        NaN   \n",
       "1            2  #CoronaVirusIndonesia \\n#CoronaBukaBorokRezim ...        NaN   \n",
       "2            3  Chinaâs wildlife trade via @SCMPgraphics  ht...        NaN   \n",
       "3            4  Proof that the media such as @CNN is unnecessa...        NaN   \n",
       "4            5  Indians are a shitty breed of humans. With the...        NaN   \n",
       "...        ...                                                ...        ...   \n",
       "528303  526787  A really great visual explanation of the pande...        NaN   \n",
       "528304  526788  Los casos de COVID-19 se registran mayoritaria...        NaN   \n",
       "528305  526789  Apa itu Virus Coronavirus (Covid-19)?\\n\\nSumbe...        NaN   \n",
       "528306  526790  Folks upset at the public preparedness measure...        NaN   \n",
       "528307  526791  See that it is empty Italy and the governmentâ...        NaN   \n",
       "\n",
       "       Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8  \\\n",
       "0             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "528303        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528304        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528305        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528306        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528307        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "       Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13  \n",
       "0             NaN         NaN         NaN         NaN         NaN  \n",
       "1             NaN         NaN         NaN         NaN         NaN  \n",
       "2             NaN         NaN         NaN         NaN         NaN  \n",
       "3             NaN         NaN         NaN         NaN         NaN  \n",
       "4             NaN         NaN         NaN         NaN         NaN  \n",
       "...           ...         ...         ...         ...         ...  \n",
       "528303        NaN         NaN         NaN         NaN         NaN  \n",
       "528304        NaN         NaN         NaN         NaN         NaN  \n",
       "528305        NaN         NaN         NaN         NaN         NaN  \n",
       "528306        NaN         NaN         NaN         NaN         NaN  \n",
       "528307        NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[528308 rows x 14 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CoronaVirusIndonesia   CoronaBukaBorokRezim  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CoronaVirusIndonesia   CoronaBukaBorokRezim  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>China   s wildlife trade via SCMPgraphics     ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Proof that the media such as CNN is unnecessar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Indians are a shitty breed of humans  With the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528303</th>\n",
       "      <td>526787</td>\n",
       "      <td>A really great visual explanation of the pande...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528304</th>\n",
       "      <td>526788</td>\n",
       "      <td>Los casos de COVID    se registran mayoritaria...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528305</th>\n",
       "      <td>526789</td>\n",
       "      <td>Apa itu Virus Coronavirus  Covid       Sumber ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528306</th>\n",
       "      <td>526790</td>\n",
       "      <td>Folks upset at the public preparedness measure...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528307</th>\n",
       "      <td>526791</td>\n",
       "      <td>See that it is empty Italy and the government ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>528308 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ItemID                                      SentimentText Unnamed: 2  \\\n",
       "0            1   CoronaVirusIndonesia   CoronaBukaBorokRezim  ...        NaN   \n",
       "1            2   CoronaVirusIndonesia   CoronaBukaBorokRezim  ...        NaN   \n",
       "2            3  China   s wildlife trade via SCMPgraphics     ...        NaN   \n",
       "3            4  Proof that the media such as CNN is unnecessar...        NaN   \n",
       "4            5  Indians are a shitty breed of humans  With the...        NaN   \n",
       "...        ...                                                ...        ...   \n",
       "528303  526787  A really great visual explanation of the pande...        NaN   \n",
       "528304  526788  Los casos de COVID    se registran mayoritaria...        NaN   \n",
       "528305  526789  Apa itu Virus Coronavirus  Covid       Sumber ...        NaN   \n",
       "528306  526790  Folks upset at the public preparedness measure...        NaN   \n",
       "528307  526791  See that it is empty Italy and the government ...        NaN   \n",
       "\n",
       "       Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8  \\\n",
       "0             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4             NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "528303        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528304        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528305        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528306        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "528307        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "       Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13  \n",
       "0             NaN         NaN         NaN         NaN         NaN  \n",
       "1             NaN         NaN         NaN         NaN         NaN  \n",
       "2             NaN         NaN         NaN         NaN         NaN  \n",
       "3             NaN         NaN         NaN         NaN         NaN  \n",
       "4             NaN         NaN         NaN         NaN         NaN  \n",
       "...           ...         ...         ...         ...         ...  \n",
       "528303        NaN         NaN         NaN         NaN         NaN  \n",
       "528304        NaN         NaN         NaN         NaN         NaN  \n",
       "528305        NaN         NaN         NaN         NaN         NaN  \n",
       "528306        NaN         NaN         NaN         NaN         NaN  \n",
       "528307        NaN         NaN         NaN         NaN         NaN  \n",
       "\n",
       "[528308 rows x 14 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-ce69a05e79c4>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-ce69a05e79c4>\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    data\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('./20200305 Coronavirus Tweets.CSV' ,encoding='ISO-8859-1',low_memory=False)\n",
    "# Removing Twitter Handles (@user)\n",
    "data['SentimentText'] = data['SentimentText'].str.replace(\"@\", \"\") \n",
    "# Removing links\n",
    "data['SentimentText'] = data['SentimentText'].str.replace(r\"http\\S+\", \"\") \n",
    "# Removing Punctuations, Numbers, and Special Characters\n",
    "data['SentimentText'] = data['SentimentText'].str.replace(\"[^a-zA-Z]\", \" \") \n",
    "# Remove stop words\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    clean_text=' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return clean_text\n",
    "data['SentimentText'] = data['SentimentText'].apply(lambda text : remove_stopwords(text.lower())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 1452: expected str instance, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-956d2f2d0ba8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnewdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnewdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ItemID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnewdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SentimentText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m110\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 1452: expected str instance, float found"
     ]
    }
   ],
   "source": [
    "newdata = copy.deepcopy(data)\n",
    "newdata.drop(['ItemID'],axis = 1,inplace = True)\n",
    "all_words = ' '.join([text for text in newdata['SentimentText']])\n",
    "wordcloud = WordCloud(width=800,height=500,random_state=21,max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
